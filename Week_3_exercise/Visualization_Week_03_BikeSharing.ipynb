{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339ee967",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Nowadays Sharing Economies become ever more present, so that for instance people start preferring to flexibly rent a car whenever they need one instead of owning a car which stands still most of the time. This also transfers to services like bike and e-scooter sharing.\n",
    "In this weeks tutorial we will use such a [Bikesharing](https://www.kaggle.com/competitions/bike-sharing-demand/) dataset and take a closer look at it by visualizing the data. This will help us gain a better overview of the dataset, identify patterns and trends, and develop a more intuitive understanding of the underlying relationships before moving on to further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6efbf",
   "metadata": {},
   "source": [
    "# Data Visualization with PyGWalker\n",
    "\n",
    "**PyGWalker** is a Python library designed for exploratory **visual data analysis**. It takes a standard Pandas DataFrame and transforms it into an interactive, drag-and-drop style interface — similar to tools like Tableau — allowing analysts to visually explore their data without having to write extensive visualization code.\n",
    "\n",
    "We’ll use it in this session because it makes it easy to quickly generate visualizations such as charts, histograms, scatter plots, heatmaps, and facet views simply by dragging variables into place. In addition, it provides a built-in data preview and profiling table, which helps us get an immediate sense of the data’s distributions, missing values, and variable types — all in one clear and interactive interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a3fb7",
   "metadata": {},
   "source": [
    "# Import necessary Libraries\n",
    "\n",
    "In this code block, we start by importing the libraries we’ll need for our work. **os** helps us interact with the file system (e.g., loading files), **pandas** is used for handling and analyzing data, **numpy** allows us to efficiently work with numerical data and perform mathematical operations, and **pygwalker** provides an easy way to visually explore datasets. We also import random to generate random values when needed. Finally, we set a random seed (both for NumPy and Python’s random module) to make sure that any random processes are reproducible — meaning we’ll get the same results each time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72725f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygwalker -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99251af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygwalker as pyg\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)  # Set seed for NumPy\n",
    "random.seed(42) # Set seed for random module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c1ff5",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Next, we will take a closer look at the BikeSharing dataset. To do this, we first need to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from a csv file\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/data/BikeSharing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fb66f",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "First let’s have a look at the data.\n",
    "\n",
    "We can use the `head()` function to display the first few lines of our data frame.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbee5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ca80c",
   "metadata": {},
   "source": [
    "Additionally we can also use the `describe()` function to get an overview of the columns of our data frame and basic descriptive statistics for the numeric columns.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfe74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c783dfc",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with PyGWalker\n",
    "\n",
    "Now we initialize PyGWalker on our dataset by calling pyg.walk(df). This opens an interactive visualization interface directly inside the notebook, where we can explore the data without writing additional plotting code.\n",
    "\n",
    "After executing the code chunk below the pygwalker UI opens and we can see the tabs \"Data\", \"Visualization\" and \"Chat\".\n",
    "\n",
    "**\"Data\" tab:** In this tab we can see the raw data, which our dataset consists of. Here we can also modify the data type of the columns. For example, click on the **blue icon** in front of **datetime** to open the options for that column. As this column contains dates we can change the data type to **temporal** to account for it.\n",
    "\n",
    "**\"Visualization\" tab:** Now we can switch to the visualization tab and start exploring the distribution of the data and the relationship between the features.\n",
    "Interesting plots might be:\n",
    "1. A bar chart showing the total bike rentals per season.\n",
    "2. A bar chart showing the total bike rentals per weather category colored by season.\n",
    "3. Plots displaying the mean and total count of bike rentals depending on if it is a holiday or not.\n",
    "4. Plots displaying the mean and total count of bike rentals depending on if it is a workingday or not.\n",
    "5. Find some visualizations that show interesting properties of the data on your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7513b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygwalker on data\n",
    "walker = pyg.walk(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27fa0e",
   "metadata": {},
   "source": [
    "**Visualization of aggregated data to see a different level of detail**\n",
    "\n",
    "Sometimes you might be interested in relations that are visible on a different level of detail. Below we aggregate the data per day and get either the averages or sums per day. \n",
    "In the pygwalker UI we can now for example display the timeseries of total bike rentals per day.\n",
    "Feel free to experiment with different visualizations as well.\n",
    "\n",
    "Further interesting patterns might be observable by aggregating per month or per weekday. Sadly, we have to write a bit of code ourself so we can visualize this in pygwalker.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b6961",
   "metadata": {},
   "source": [
    "Here we take the **hourly bike rental data** and turn it into a **daily summary** that’s easier to analyze. It first converts the 'datetime' column into a proper date format, then resamples the data by day to calculate total and average values. The **totals** capture how many **rides happened each day**, while the **averages** describe the day’s typical **weather** and **rental patterns**. After that, it cleans up the date column and resets the index so the dataset is neat and ready for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data_day = pd.DataFrame()\n",
    "# Aggregate hourly data to daily data \n",
    "data_day = data.resample('D', on='datetime')[[\"count\", \"casual\", \"registered\"]].sum().rename(columns={\"count\": \"total_count\", \"casual\": \"total_casual\", \"registered\": \"total_registered\"})\n",
    "data_day = data.resample('D', on='datetime')[[\"temp\", \"atemp\", \"humidity\", \"windspeed\", \"count\", \"casual\", \"registered\"]].mean().rename(columns={\"temp\": \"avg_temp\", \"atemp\": \"avg_atemp\", \"humidity\": \"avg_humidity\", \"windspeed\": \"avg_windspeed\", \"count\": \"hourly_avg_count\", \"casual\": \"hourly_avg_casual\", \"registered\": \"hourly_avg_registered\"}).join(data_day)\n",
    "# Get date from index\n",
    "data_day['date'] = data_day.index.date\n",
    "# Reset index\n",
    "data_day = data_day.reset_index(drop=True)\n",
    "walker = pyg.walk(data_day)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
